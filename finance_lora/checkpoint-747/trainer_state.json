{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 747,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008032128514056224,
      "grad_norm": 0.6418671607971191,
      "learning_rate": 4.99330655957162e-05,
      "loss": 4.5993,
      "step": 2
    },
    {
      "epoch": 0.01606425702811245,
      "grad_norm": 0.7926599383354187,
      "learning_rate": 4.97991967871486e-05,
      "loss": 4.6242,
      "step": 4
    },
    {
      "epoch": 0.024096385542168676,
      "grad_norm": 0.9919983148574829,
      "learning_rate": 4.9665327978580993e-05,
      "loss": 4.5651,
      "step": 6
    },
    {
      "epoch": 0.0321285140562249,
      "grad_norm": 0.7289333939552307,
      "learning_rate": 4.953145917001339e-05,
      "loss": 3.9811,
      "step": 8
    },
    {
      "epoch": 0.040160642570281124,
      "grad_norm": 0.8043560981750488,
      "learning_rate": 4.9397590361445786e-05,
      "loss": 5.0565,
      "step": 10
    },
    {
      "epoch": 0.04819277108433735,
      "grad_norm": 0.656496524810791,
      "learning_rate": 4.926372155287818e-05,
      "loss": 4.5092,
      "step": 12
    },
    {
      "epoch": 0.05622489959839357,
      "grad_norm": 1.0195444822311401,
      "learning_rate": 4.912985274431058e-05,
      "loss": 5.538,
      "step": 14
    },
    {
      "epoch": 0.0642570281124498,
      "grad_norm": 1.0158905982971191,
      "learning_rate": 4.8995983935742975e-05,
      "loss": 6.0612,
      "step": 16
    },
    {
      "epoch": 0.07228915662650602,
      "grad_norm": 0.8688011169433594,
      "learning_rate": 4.886211512717537e-05,
      "loss": 4.3211,
      "step": 18
    },
    {
      "epoch": 0.08032128514056225,
      "grad_norm": 0.9486854672431946,
      "learning_rate": 4.872824631860777e-05,
      "loss": 4.756,
      "step": 20
    },
    {
      "epoch": 0.08835341365461848,
      "grad_norm": 0.8081942200660706,
      "learning_rate": 4.8594377510040165e-05,
      "loss": 4.4171,
      "step": 22
    },
    {
      "epoch": 0.0963855421686747,
      "grad_norm": 0.7607070207595825,
      "learning_rate": 4.8460508701472554e-05,
      "loss": 4.474,
      "step": 24
    },
    {
      "epoch": 0.10441767068273092,
      "grad_norm": 0.9423701167106628,
      "learning_rate": 4.832663989290496e-05,
      "loss": 4.7132,
      "step": 26
    },
    {
      "epoch": 0.11244979919678715,
      "grad_norm": 1.0291059017181396,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 4.1312,
      "step": 28
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 0.8214288949966431,
      "learning_rate": 4.8058902275769744e-05,
      "loss": 4.1261,
      "step": 30
    },
    {
      "epoch": 0.1285140562248996,
      "grad_norm": 0.7751452922821045,
      "learning_rate": 4.792503346720215e-05,
      "loss": 4.391,
      "step": 32
    },
    {
      "epoch": 0.13654618473895583,
      "grad_norm": 1.0733282566070557,
      "learning_rate": 4.779116465863454e-05,
      "loss": 5.5924,
      "step": 34
    },
    {
      "epoch": 0.14457831325301204,
      "grad_norm": 0.8480847477912903,
      "learning_rate": 4.765729585006693e-05,
      "loss": 4.4491,
      "step": 36
    },
    {
      "epoch": 0.15261044176706828,
      "grad_norm": 1.1552354097366333,
      "learning_rate": 4.7523427041499336e-05,
      "loss": 4.036,
      "step": 38
    },
    {
      "epoch": 0.1606425702811245,
      "grad_norm": 0.9028279781341553,
      "learning_rate": 4.738955823293173e-05,
      "loss": 5.4435,
      "step": 40
    },
    {
      "epoch": 0.1686746987951807,
      "grad_norm": 1.106876015663147,
      "learning_rate": 4.725568942436412e-05,
      "loss": 4.6358,
      "step": 42
    },
    {
      "epoch": 0.17670682730923695,
      "grad_norm": 1.1708897352218628,
      "learning_rate": 4.712182061579652e-05,
      "loss": 5.1183,
      "step": 44
    },
    {
      "epoch": 0.18473895582329317,
      "grad_norm": 1.0690574645996094,
      "learning_rate": 4.698795180722892e-05,
      "loss": 4.84,
      "step": 46
    },
    {
      "epoch": 0.1927710843373494,
      "grad_norm": 1.495012640953064,
      "learning_rate": 4.685408299866131e-05,
      "loss": 5.1123,
      "step": 48
    },
    {
      "epoch": 0.20080321285140562,
      "grad_norm": 1.0771349668502808,
      "learning_rate": 4.672021419009371e-05,
      "loss": 4.8483,
      "step": 50
    },
    {
      "epoch": 0.20883534136546184,
      "grad_norm": 1.2273465394973755,
      "learning_rate": 4.658634538152611e-05,
      "loss": 4.7145,
      "step": 52
    },
    {
      "epoch": 0.21686746987951808,
      "grad_norm": 1.0654946565628052,
      "learning_rate": 4.64524765729585e-05,
      "loss": 4.4194,
      "step": 54
    },
    {
      "epoch": 0.2248995983935743,
      "grad_norm": 1.114250659942627,
      "learning_rate": 4.63186077643909e-05,
      "loss": 5.1248,
      "step": 56
    },
    {
      "epoch": 0.23293172690763053,
      "grad_norm": 1.1828900575637817,
      "learning_rate": 4.61847389558233e-05,
      "loss": 4.2135,
      "step": 58
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 1.2733561992645264,
      "learning_rate": 4.605087014725569e-05,
      "loss": 5.1228,
      "step": 60
    },
    {
      "epoch": 0.24899598393574296,
      "grad_norm": 1.6130532026290894,
      "learning_rate": 4.5917001338688086e-05,
      "loss": 4.7407,
      "step": 62
    },
    {
      "epoch": 0.2570281124497992,
      "grad_norm": 1.4969642162322998,
      "learning_rate": 4.578313253012048e-05,
      "loss": 4.6026,
      "step": 64
    },
    {
      "epoch": 0.26506024096385544,
      "grad_norm": 1.0826725959777832,
      "learning_rate": 4.564926372155288e-05,
      "loss": 4.7776,
      "step": 66
    },
    {
      "epoch": 0.27309236947791166,
      "grad_norm": 1.0318838357925415,
      "learning_rate": 4.5515394912985275e-05,
      "loss": 4.5201,
      "step": 68
    },
    {
      "epoch": 0.28112449799196787,
      "grad_norm": 1.3740206956863403,
      "learning_rate": 4.538152610441767e-05,
      "loss": 5.0622,
      "step": 70
    },
    {
      "epoch": 0.2891566265060241,
      "grad_norm": 1.5151194334030151,
      "learning_rate": 4.524765729585007e-05,
      "loss": 4.7513,
      "step": 72
    },
    {
      "epoch": 0.2971887550200803,
      "grad_norm": 1.4209976196289062,
      "learning_rate": 4.5113788487282465e-05,
      "loss": 4.6421,
      "step": 74
    },
    {
      "epoch": 0.30522088353413657,
      "grad_norm": 0.9752093553543091,
      "learning_rate": 4.497991967871486e-05,
      "loss": 4.5005,
      "step": 76
    },
    {
      "epoch": 0.3132530120481928,
      "grad_norm": 1.438794493675232,
      "learning_rate": 4.484605087014726e-05,
      "loss": 4.1317,
      "step": 78
    },
    {
      "epoch": 0.321285140562249,
      "grad_norm": 1.1610885858535767,
      "learning_rate": 4.4712182061579654e-05,
      "loss": 4.584,
      "step": 80
    },
    {
      "epoch": 0.3293172690763052,
      "grad_norm": 1.2854164838790894,
      "learning_rate": 4.457831325301205e-05,
      "loss": 4.7928,
      "step": 82
    },
    {
      "epoch": 0.3373493975903614,
      "grad_norm": 1.1986840963363647,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 4.4356,
      "step": 84
    },
    {
      "epoch": 0.3453815261044177,
      "grad_norm": 2.0115933418273926,
      "learning_rate": 4.431057563587684e-05,
      "loss": 4.8831,
      "step": 86
    },
    {
      "epoch": 0.3534136546184739,
      "grad_norm": 1.5651277303695679,
      "learning_rate": 4.417670682730924e-05,
      "loss": 5.3215,
      "step": 88
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 1.734904408454895,
      "learning_rate": 4.4042838018741636e-05,
      "loss": 4.584,
      "step": 90
    },
    {
      "epoch": 0.36947791164658633,
      "grad_norm": 1.7477623224258423,
      "learning_rate": 4.390896921017403e-05,
      "loss": 3.8351,
      "step": 92
    },
    {
      "epoch": 0.37751004016064255,
      "grad_norm": 1.53696870803833,
      "learning_rate": 4.377510040160643e-05,
      "loss": 3.617,
      "step": 94
    },
    {
      "epoch": 0.3855421686746988,
      "grad_norm": 1.4230231046676636,
      "learning_rate": 4.3641231593038825e-05,
      "loss": 4.1337,
      "step": 96
    },
    {
      "epoch": 0.39357429718875503,
      "grad_norm": 1.6773072481155396,
      "learning_rate": 4.350736278447122e-05,
      "loss": 4.6904,
      "step": 98
    },
    {
      "epoch": 0.40160642570281124,
      "grad_norm": 1.8647021055221558,
      "learning_rate": 4.337349397590362e-05,
      "loss": 4.0994,
      "step": 100
    },
    {
      "epoch": 0.40963855421686746,
      "grad_norm": 1.4652146100997925,
      "learning_rate": 4.3239625167336014e-05,
      "loss": 4.3001,
      "step": 102
    },
    {
      "epoch": 0.41767068273092367,
      "grad_norm": 1.5496412515640259,
      "learning_rate": 4.3105756358768404e-05,
      "loss": 4.6059,
      "step": 104
    },
    {
      "epoch": 0.42570281124497994,
      "grad_norm": 1.681849718093872,
      "learning_rate": 4.297188755020081e-05,
      "loss": 4.9818,
      "step": 106
    },
    {
      "epoch": 0.43373493975903615,
      "grad_norm": 1.6842560768127441,
      "learning_rate": 4.2838018741633203e-05,
      "loss": 4.1679,
      "step": 108
    },
    {
      "epoch": 0.44176706827309237,
      "grad_norm": 1.7647292613983154,
      "learning_rate": 4.270414993306559e-05,
      "loss": 4.2031,
      "step": 110
    },
    {
      "epoch": 0.4497991967871486,
      "grad_norm": 1.549933671951294,
      "learning_rate": 4.2570281124497996e-05,
      "loss": 4.1969,
      "step": 112
    },
    {
      "epoch": 0.4578313253012048,
      "grad_norm": 2.032020330429077,
      "learning_rate": 4.243641231593039e-05,
      "loss": 4.0594,
      "step": 114
    },
    {
      "epoch": 0.46586345381526106,
      "grad_norm": 2.151899814605713,
      "learning_rate": 4.230254350736278e-05,
      "loss": 5.2349,
      "step": 116
    },
    {
      "epoch": 0.4738955823293173,
      "grad_norm": 1.266398310661316,
      "learning_rate": 4.2168674698795186e-05,
      "loss": 4.0818,
      "step": 118
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 1.7007286548614502,
      "learning_rate": 4.203480589022758e-05,
      "loss": 4.3112,
      "step": 120
    },
    {
      "epoch": 0.4899598393574297,
      "grad_norm": 1.4197317361831665,
      "learning_rate": 4.190093708165997e-05,
      "loss": 4.5393,
      "step": 122
    },
    {
      "epoch": 0.4979919678714859,
      "grad_norm": 1.8312739133834839,
      "learning_rate": 4.176706827309237e-05,
      "loss": 4.5181,
      "step": 124
    },
    {
      "epoch": 0.5060240963855421,
      "grad_norm": 1.555044412612915,
      "learning_rate": 4.163319946452477e-05,
      "loss": 4.1694,
      "step": 126
    },
    {
      "epoch": 0.5140562248995983,
      "grad_norm": 1.7952297925949097,
      "learning_rate": 4.149933065595716e-05,
      "loss": 4.4103,
      "step": 128
    },
    {
      "epoch": 0.5220883534136547,
      "grad_norm": 2.0938193798065186,
      "learning_rate": 4.136546184738956e-05,
      "loss": 4.9208,
      "step": 130
    },
    {
      "epoch": 0.5301204819277109,
      "grad_norm": 1.7036420106887817,
      "learning_rate": 4.123159303882196e-05,
      "loss": 3.9623,
      "step": 132
    },
    {
      "epoch": 0.5381526104417671,
      "grad_norm": 1.3835420608520508,
      "learning_rate": 4.109772423025435e-05,
      "loss": 4.3245,
      "step": 134
    },
    {
      "epoch": 0.5461847389558233,
      "grad_norm": 1.813605785369873,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 4.3409,
      "step": 136
    },
    {
      "epoch": 0.5542168674698795,
      "grad_norm": 1.4999502897262573,
      "learning_rate": 4.082998661311915e-05,
      "loss": 4.3223,
      "step": 138
    },
    {
      "epoch": 0.5622489959839357,
      "grad_norm": 2.17645525932312,
      "learning_rate": 4.069611780455154e-05,
      "loss": 3.8025,
      "step": 140
    },
    {
      "epoch": 0.570281124497992,
      "grad_norm": 2.1065728664398193,
      "learning_rate": 4.0562248995983936e-05,
      "loss": 3.9649,
      "step": 142
    },
    {
      "epoch": 0.5783132530120482,
      "grad_norm": 2.0453708171844482,
      "learning_rate": 4.042838018741633e-05,
      "loss": 4.7693,
      "step": 144
    },
    {
      "epoch": 0.5863453815261044,
      "grad_norm": 2.670196533203125,
      "learning_rate": 4.029451137884873e-05,
      "loss": 4.4533,
      "step": 146
    },
    {
      "epoch": 0.5943775100401606,
      "grad_norm": 1.5382750034332275,
      "learning_rate": 4.0160642570281125e-05,
      "loss": 3.5022,
      "step": 148
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 1.7093781232833862,
      "learning_rate": 4.002677376171352e-05,
      "loss": 4.0385,
      "step": 150
    },
    {
      "epoch": 0.6104417670682731,
      "grad_norm": 1.4471317529678345,
      "learning_rate": 3.989290495314592e-05,
      "loss": 3.9984,
      "step": 152
    },
    {
      "epoch": 0.6184738955823293,
      "grad_norm": 2.2328310012817383,
      "learning_rate": 3.9759036144578314e-05,
      "loss": 4.4386,
      "step": 154
    },
    {
      "epoch": 0.6265060240963856,
      "grad_norm": 2.1507766246795654,
      "learning_rate": 3.962516733601071e-05,
      "loss": 4.121,
      "step": 156
    },
    {
      "epoch": 0.6345381526104418,
      "grad_norm": 1.9734864234924316,
      "learning_rate": 3.949129852744311e-05,
      "loss": 4.2599,
      "step": 158
    },
    {
      "epoch": 0.642570281124498,
      "grad_norm": 2.010174512863159,
      "learning_rate": 3.93574297188755e-05,
      "loss": 4.712,
      "step": 160
    },
    {
      "epoch": 0.6506024096385542,
      "grad_norm": 2.444444179534912,
      "learning_rate": 3.92235609103079e-05,
      "loss": 4.0387,
      "step": 162
    },
    {
      "epoch": 0.6586345381526104,
      "grad_norm": 2.035673141479492,
      "learning_rate": 3.9089692101740296e-05,
      "loss": 3.9664,
      "step": 164
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.817858338356018,
      "learning_rate": 3.895582329317269e-05,
      "loss": 4.4526,
      "step": 166
    },
    {
      "epoch": 0.6746987951807228,
      "grad_norm": 2.0499379634857178,
      "learning_rate": 3.882195448460509e-05,
      "loss": 4.6681,
      "step": 168
    },
    {
      "epoch": 0.6827309236947792,
      "grad_norm": 1.9617353677749634,
      "learning_rate": 3.8688085676037485e-05,
      "loss": 4.3793,
      "step": 170
    },
    {
      "epoch": 0.6907630522088354,
      "grad_norm": 1.5514609813690186,
      "learning_rate": 3.855421686746988e-05,
      "loss": 4.2748,
      "step": 172
    },
    {
      "epoch": 0.6987951807228916,
      "grad_norm": 2.029144287109375,
      "learning_rate": 3.842034805890228e-05,
      "loss": 3.4456,
      "step": 174
    },
    {
      "epoch": 0.7068273092369478,
      "grad_norm": 2.3222923278808594,
      "learning_rate": 3.8286479250334675e-05,
      "loss": 4.3095,
      "step": 176
    },
    {
      "epoch": 0.714859437751004,
      "grad_norm": 1.83786940574646,
      "learning_rate": 3.815261044176707e-05,
      "loss": 3.6403,
      "step": 178
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 2.0331008434295654,
      "learning_rate": 3.801874163319947e-05,
      "loss": 3.8965,
      "step": 180
    },
    {
      "epoch": 0.7309236947791165,
      "grad_norm": 2.4076061248779297,
      "learning_rate": 3.7884872824631864e-05,
      "loss": 4.1486,
      "step": 182
    },
    {
      "epoch": 0.7389558232931727,
      "grad_norm": 2.3866312503814697,
      "learning_rate": 3.7751004016064253e-05,
      "loss": 4.1781,
      "step": 184
    },
    {
      "epoch": 0.7469879518072289,
      "grad_norm": 1.6872074604034424,
      "learning_rate": 3.761713520749666e-05,
      "loss": 4.0673,
      "step": 186
    },
    {
      "epoch": 0.7550200803212851,
      "grad_norm": 1.9115878343582153,
      "learning_rate": 3.748326639892905e-05,
      "loss": 4.2022,
      "step": 188
    },
    {
      "epoch": 0.7630522088353414,
      "grad_norm": 2.5999553203582764,
      "learning_rate": 3.734939759036144e-05,
      "loss": 4.5564,
      "step": 190
    },
    {
      "epoch": 0.7710843373493976,
      "grad_norm": 1.9424901008605957,
      "learning_rate": 3.7215528781793846e-05,
      "loss": 4.6336,
      "step": 192
    },
    {
      "epoch": 0.7791164658634538,
      "grad_norm": 2.3178303241729736,
      "learning_rate": 3.708165997322624e-05,
      "loss": 4.0364,
      "step": 194
    },
    {
      "epoch": 0.7871485943775101,
      "grad_norm": 1.9963840246200562,
      "learning_rate": 3.694779116465863e-05,
      "loss": 4.4565,
      "step": 196
    },
    {
      "epoch": 0.7951807228915663,
      "grad_norm": 1.7181380987167358,
      "learning_rate": 3.6813922356091035e-05,
      "loss": 4.0375,
      "step": 198
    },
    {
      "epoch": 0.8032128514056225,
      "grad_norm": 2.2402944564819336,
      "learning_rate": 3.668005354752343e-05,
      "loss": 4.0921,
      "step": 200
    },
    {
      "epoch": 0.8112449799196787,
      "grad_norm": 2.2417831420898438,
      "learning_rate": 3.654618473895582e-05,
      "loss": 3.7078,
      "step": 202
    },
    {
      "epoch": 0.8192771084337349,
      "grad_norm": 2.3700151443481445,
      "learning_rate": 3.641231593038822e-05,
      "loss": 4.2134,
      "step": 204
    },
    {
      "epoch": 0.8273092369477911,
      "grad_norm": 2.602571487426758,
      "learning_rate": 3.627844712182062e-05,
      "loss": 3.8683,
      "step": 206
    },
    {
      "epoch": 0.8353413654618473,
      "grad_norm": 2.4226691722869873,
      "learning_rate": 3.614457831325301e-05,
      "loss": 4.3827,
      "step": 208
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 1.8206638097763062,
      "learning_rate": 3.601070950468541e-05,
      "loss": 4.2241,
      "step": 210
    },
    {
      "epoch": 0.8514056224899599,
      "grad_norm": 2.230893850326538,
      "learning_rate": 3.587684069611781e-05,
      "loss": 4.041,
      "step": 212
    },
    {
      "epoch": 0.8594377510040161,
      "grad_norm": 2.924481153488159,
      "learning_rate": 3.57429718875502e-05,
      "loss": 4.3547,
      "step": 214
    },
    {
      "epoch": 0.8674698795180723,
      "grad_norm": 1.9551072120666504,
      "learning_rate": 3.5609103078982596e-05,
      "loss": 4.1171,
      "step": 216
    },
    {
      "epoch": 0.8755020080321285,
      "grad_norm": 2.8053336143493652,
      "learning_rate": 3.5475234270415e-05,
      "loss": 4.8191,
      "step": 218
    },
    {
      "epoch": 0.8835341365461847,
      "grad_norm": 1.9079523086547852,
      "learning_rate": 3.534136546184739e-05,
      "loss": 3.8364,
      "step": 220
    },
    {
      "epoch": 0.891566265060241,
      "grad_norm": 2.927971601486206,
      "learning_rate": 3.5207496653279785e-05,
      "loss": 3.8633,
      "step": 222
    },
    {
      "epoch": 0.8995983935742972,
      "grad_norm": 2.519166946411133,
      "learning_rate": 3.507362784471218e-05,
      "loss": 4.6849,
      "step": 224
    },
    {
      "epoch": 0.9076305220883534,
      "grad_norm": 2.329669952392578,
      "learning_rate": 3.4939759036144585e-05,
      "loss": 4.2376,
      "step": 226
    },
    {
      "epoch": 0.9156626506024096,
      "grad_norm": 2.3601059913635254,
      "learning_rate": 3.4805890227576974e-05,
      "loss": 3.2477,
      "step": 228
    },
    {
      "epoch": 0.9236947791164659,
      "grad_norm": 3.0549376010894775,
      "learning_rate": 3.467202141900937e-05,
      "loss": 4.1823,
      "step": 230
    },
    {
      "epoch": 0.9317269076305221,
      "grad_norm": 2.0988619327545166,
      "learning_rate": 3.4538152610441774e-05,
      "loss": 3.9112,
      "step": 232
    },
    {
      "epoch": 0.9397590361445783,
      "grad_norm": 3.5524187088012695,
      "learning_rate": 3.4404283801874164e-05,
      "loss": 3.6499,
      "step": 234
    },
    {
      "epoch": 0.9477911646586346,
      "grad_norm": 2.137972593307495,
      "learning_rate": 3.427041499330656e-05,
      "loss": 4.4146,
      "step": 236
    },
    {
      "epoch": 0.9558232931726908,
      "grad_norm": 2.112632989883423,
      "learning_rate": 3.413654618473896e-05,
      "loss": 3.3734,
      "step": 238
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 2.3097548484802246,
      "learning_rate": 3.400267737617135e-05,
      "loss": 4.5252,
      "step": 240
    },
    {
      "epoch": 0.9718875502008032,
      "grad_norm": 1.8135488033294678,
      "learning_rate": 3.386880856760375e-05,
      "loss": 4.3891,
      "step": 242
    },
    {
      "epoch": 0.9799196787148594,
      "grad_norm": 2.8087809085845947,
      "learning_rate": 3.3734939759036146e-05,
      "loss": 3.6971,
      "step": 244
    },
    {
      "epoch": 0.9879518072289156,
      "grad_norm": 2.0662195682525635,
      "learning_rate": 3.360107095046854e-05,
      "loss": 3.9489,
      "step": 246
    },
    {
      "epoch": 0.9959839357429718,
      "grad_norm": 2.5366973876953125,
      "learning_rate": 3.346720214190094e-05,
      "loss": 4.4307,
      "step": 248
    },
    {
      "epoch": 1.0040160642570282,
      "grad_norm": 2.062514066696167,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.0803,
      "step": 250
    },
    {
      "epoch": 1.0120481927710843,
      "grad_norm": 2.682870864868164,
      "learning_rate": 3.319946452476573e-05,
      "loss": 3.6591,
      "step": 252
    },
    {
      "epoch": 1.0200803212851406,
      "grad_norm": 2.16640043258667,
      "learning_rate": 3.306559571619813e-05,
      "loss": 3.913,
      "step": 254
    },
    {
      "epoch": 1.0281124497991967,
      "grad_norm": 3.0444176197052,
      "learning_rate": 3.2931726907630524e-05,
      "loss": 3.7181,
      "step": 256
    },
    {
      "epoch": 1.036144578313253,
      "grad_norm": 2.2397782802581787,
      "learning_rate": 3.279785809906292e-05,
      "loss": 4.3598,
      "step": 258
    },
    {
      "epoch": 1.0441767068273093,
      "grad_norm": 2.235830307006836,
      "learning_rate": 3.266398929049532e-05,
      "loss": 4.4617,
      "step": 260
    },
    {
      "epoch": 1.0522088353413654,
      "grad_norm": 2.527146100997925,
      "learning_rate": 3.253012048192771e-05,
      "loss": 3.8524,
      "step": 262
    },
    {
      "epoch": 1.0602409638554218,
      "grad_norm": 2.1083827018737793,
      "learning_rate": 3.23962516733601e-05,
      "loss": 4.3716,
      "step": 264
    },
    {
      "epoch": 1.0682730923694779,
      "grad_norm": 2.506040334701538,
      "learning_rate": 3.2262382864792506e-05,
      "loss": 4.0152,
      "step": 266
    },
    {
      "epoch": 1.0763052208835342,
      "grad_norm": 2.5285513401031494,
      "learning_rate": 3.21285140562249e-05,
      "loss": 3.6496,
      "step": 268
    },
    {
      "epoch": 1.0843373493975903,
      "grad_norm": 3.256946086883545,
      "learning_rate": 3.199464524765729e-05,
      "loss": 4.5719,
      "step": 270
    },
    {
      "epoch": 1.0923694779116466,
      "grad_norm": 2.133741617202759,
      "learning_rate": 3.1860776439089695e-05,
      "loss": 3.9425,
      "step": 272
    },
    {
      "epoch": 1.1004016064257027,
      "grad_norm": 2.9149558544158936,
      "learning_rate": 3.172690763052209e-05,
      "loss": 5.1326,
      "step": 274
    },
    {
      "epoch": 1.108433734939759,
      "grad_norm": 2.7451047897338867,
      "learning_rate": 3.159303882195448e-05,
      "loss": 3.946,
      "step": 276
    },
    {
      "epoch": 1.1164658634538154,
      "grad_norm": 2.2106540203094482,
      "learning_rate": 3.1459170013386885e-05,
      "loss": 3.7676,
      "step": 278
    },
    {
      "epoch": 1.1244979919678715,
      "grad_norm": 2.0951902866363525,
      "learning_rate": 3.132530120481928e-05,
      "loss": 3.8124,
      "step": 280
    },
    {
      "epoch": 1.1325301204819278,
      "grad_norm": 2.0467529296875,
      "learning_rate": 3.119143239625168e-05,
      "loss": 4.8696,
      "step": 282
    },
    {
      "epoch": 1.140562248995984,
      "grad_norm": 2.4436323642730713,
      "learning_rate": 3.105756358768407e-05,
      "loss": 4.7982,
      "step": 284
    },
    {
      "epoch": 1.1485943775100402,
      "grad_norm": 3.221096992492676,
      "learning_rate": 3.092369477911647e-05,
      "loss": 3.5269,
      "step": 286
    },
    {
      "epoch": 1.1566265060240963,
      "grad_norm": 2.767756462097168,
      "learning_rate": 3.078982597054887e-05,
      "loss": 5.3934,
      "step": 288
    },
    {
      "epoch": 1.1646586345381527,
      "grad_norm": 2.772782325744629,
      "learning_rate": 3.0655957161981256e-05,
      "loss": 4.1031,
      "step": 290
    },
    {
      "epoch": 1.1726907630522088,
      "grad_norm": 2.7268643379211426,
      "learning_rate": 3.052208835341366e-05,
      "loss": 3.4714,
      "step": 292
    },
    {
      "epoch": 1.180722891566265,
      "grad_norm": 2.414691925048828,
      "learning_rate": 3.0388219544846053e-05,
      "loss": 3.3567,
      "step": 294
    },
    {
      "epoch": 1.1887550200803212,
      "grad_norm": 3.00791335105896,
      "learning_rate": 3.025435073627845e-05,
      "loss": 3.8032,
      "step": 296
    },
    {
      "epoch": 1.1967871485943775,
      "grad_norm": 2.452033281326294,
      "learning_rate": 3.012048192771085e-05,
      "loss": 3.742,
      "step": 298
    },
    {
      "epoch": 1.2048192771084336,
      "grad_norm": 2.5804100036621094,
      "learning_rate": 2.9986613119143242e-05,
      "loss": 4.1681,
      "step": 300
    },
    {
      "epoch": 1.21285140562249,
      "grad_norm": 3.019808292388916,
      "learning_rate": 2.9852744310575638e-05,
      "loss": 4.3212,
      "step": 302
    },
    {
      "epoch": 1.2208835341365463,
      "grad_norm": 2.4141454696655273,
      "learning_rate": 2.971887550200803e-05,
      "loss": 3.5912,
      "step": 304
    },
    {
      "epoch": 1.2289156626506024,
      "grad_norm": 3.6013057231903076,
      "learning_rate": 2.958500669344043e-05,
      "loss": 4.0439,
      "step": 306
    },
    {
      "epoch": 1.2369477911646587,
      "grad_norm": 2.439150810241699,
      "learning_rate": 2.9451137884872827e-05,
      "loss": 4.1524,
      "step": 308
    },
    {
      "epoch": 1.2449799196787148,
      "grad_norm": 3.7459750175476074,
      "learning_rate": 2.931726907630522e-05,
      "loss": 3.8122,
      "step": 310
    },
    {
      "epoch": 1.2530120481927711,
      "grad_norm": 1.8367103338241577,
      "learning_rate": 2.918340026773762e-05,
      "loss": 3.536,
      "step": 312
    },
    {
      "epoch": 1.2610441767068274,
      "grad_norm": 2.2153968811035156,
      "learning_rate": 2.9049531459170017e-05,
      "loss": 3.7091,
      "step": 314
    },
    {
      "epoch": 1.2690763052208835,
      "grad_norm": 2.2805652618408203,
      "learning_rate": 2.891566265060241e-05,
      "loss": 3.9837,
      "step": 316
    },
    {
      "epoch": 1.2771084337349397,
      "grad_norm": 2.7831881046295166,
      "learning_rate": 2.878179384203481e-05,
      "loss": 4.0536,
      "step": 318
    },
    {
      "epoch": 1.285140562248996,
      "grad_norm": 2.2148492336273193,
      "learning_rate": 2.8647925033467206e-05,
      "loss": 3.7009,
      "step": 320
    },
    {
      "epoch": 1.2931726907630523,
      "grad_norm": 2.290839195251465,
      "learning_rate": 2.85140562248996e-05,
      "loss": 3.8536,
      "step": 322
    },
    {
      "epoch": 1.3012048192771084,
      "grad_norm": 2.7308053970336914,
      "learning_rate": 2.8380187416331995e-05,
      "loss": 4.0076,
      "step": 324
    },
    {
      "epoch": 1.3092369477911647,
      "grad_norm": 2.9896011352539062,
      "learning_rate": 2.8246318607764395e-05,
      "loss": 4.0364,
      "step": 326
    },
    {
      "epoch": 1.3172690763052208,
      "grad_norm": 2.907655954360962,
      "learning_rate": 2.8112449799196788e-05,
      "loss": 3.7186,
      "step": 328
    },
    {
      "epoch": 1.3253012048192772,
      "grad_norm": 2.97383189201355,
      "learning_rate": 2.7978580990629184e-05,
      "loss": 4.1068,
      "step": 330
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.731783151626587,
      "learning_rate": 2.7844712182061584e-05,
      "loss": 3.6377,
      "step": 332
    },
    {
      "epoch": 1.3413654618473896,
      "grad_norm": 2.332520008087158,
      "learning_rate": 2.7710843373493977e-05,
      "loss": 4.5481,
      "step": 334
    },
    {
      "epoch": 1.3493975903614457,
      "grad_norm": 2.543055772781372,
      "learning_rate": 2.7576974564926374e-05,
      "loss": 3.9176,
      "step": 336
    },
    {
      "epoch": 1.357429718875502,
      "grad_norm": 2.7994632720947266,
      "learning_rate": 2.7443105756358774e-05,
      "loss": 3.8872,
      "step": 338
    },
    {
      "epoch": 1.3654618473895583,
      "grad_norm": 2.554851770401001,
      "learning_rate": 2.7309236947791167e-05,
      "loss": 4.1881,
      "step": 340
    },
    {
      "epoch": 1.3734939759036144,
      "grad_norm": 3.605499029159546,
      "learning_rate": 2.7175368139223563e-05,
      "loss": 3.5016,
      "step": 342
    },
    {
      "epoch": 1.3815261044176708,
      "grad_norm": 2.0887277126312256,
      "learning_rate": 2.7041499330655956e-05,
      "loss": 3.9616,
      "step": 344
    },
    {
      "epoch": 1.3895582329317269,
      "grad_norm": 2.26527738571167,
      "learning_rate": 2.6907630522088356e-05,
      "loss": 3.5813,
      "step": 346
    },
    {
      "epoch": 1.3975903614457832,
      "grad_norm": 3.2861485481262207,
      "learning_rate": 2.6773761713520752e-05,
      "loss": 4.0988,
      "step": 348
    },
    {
      "epoch": 1.4056224899598393,
      "grad_norm": 3.832831382751465,
      "learning_rate": 2.6639892904953145e-05,
      "loss": 4.7137,
      "step": 350
    },
    {
      "epoch": 1.4136546184738956,
      "grad_norm": 2.6139979362487793,
      "learning_rate": 2.6506024096385545e-05,
      "loss": 3.4167,
      "step": 352
    },
    {
      "epoch": 1.4216867469879517,
      "grad_norm": 2.9579408168792725,
      "learning_rate": 2.637215528781794e-05,
      "loss": 4.2432,
      "step": 354
    },
    {
      "epoch": 1.429718875502008,
      "grad_norm": 2.8848607540130615,
      "learning_rate": 2.6238286479250334e-05,
      "loss": 3.8057,
      "step": 356
    },
    {
      "epoch": 1.4377510040160644,
      "grad_norm": 3.786083698272705,
      "learning_rate": 2.6104417670682734e-05,
      "loss": 4.1832,
      "step": 358
    },
    {
      "epoch": 1.4457831325301205,
      "grad_norm": 3.012760877609253,
      "learning_rate": 2.597054886211513e-05,
      "loss": 4.2767,
      "step": 360
    },
    {
      "epoch": 1.4538152610441766,
      "grad_norm": 2.7633678913116455,
      "learning_rate": 2.5836680053547524e-05,
      "loss": 3.9054,
      "step": 362
    },
    {
      "epoch": 1.461847389558233,
      "grad_norm": 2.957231283187866,
      "learning_rate": 2.570281124497992e-05,
      "loss": 4.3578,
      "step": 364
    },
    {
      "epoch": 1.4698795180722892,
      "grad_norm": 3.542914390563965,
      "learning_rate": 2.556894243641232e-05,
      "loss": 4.4509,
      "step": 366
    },
    {
      "epoch": 1.4779116465863453,
      "grad_norm": 2.555479049682617,
      "learning_rate": 2.5435073627844713e-05,
      "loss": 3.1399,
      "step": 368
    },
    {
      "epoch": 1.4859437751004017,
      "grad_norm": 2.925633430480957,
      "learning_rate": 2.530120481927711e-05,
      "loss": 4.2389,
      "step": 370
    },
    {
      "epoch": 1.4939759036144578,
      "grad_norm": 2.628535270690918,
      "learning_rate": 2.516733601070951e-05,
      "loss": 3.8846,
      "step": 372
    },
    {
      "epoch": 1.502008032128514,
      "grad_norm": 3.2078986167907715,
      "learning_rate": 2.5033467202141902e-05,
      "loss": 3.1664,
      "step": 374
    },
    {
      "epoch": 1.5100401606425704,
      "grad_norm": 2.1775741577148438,
      "learning_rate": 2.48995983935743e-05,
      "loss": 4.4336,
      "step": 376
    },
    {
      "epoch": 1.5180722891566265,
      "grad_norm": 3.5339548587799072,
      "learning_rate": 2.4765729585006695e-05,
      "loss": 4.0028,
      "step": 378
    },
    {
      "epoch": 1.5261044176706826,
      "grad_norm": 2.3854057788848877,
      "learning_rate": 2.463186077643909e-05,
      "loss": 4.2122,
      "step": 380
    },
    {
      "epoch": 1.534136546184739,
      "grad_norm": 3.2234861850738525,
      "learning_rate": 2.4497991967871488e-05,
      "loss": 4.3592,
      "step": 382
    },
    {
      "epoch": 1.5421686746987953,
      "grad_norm": 2.112927198410034,
      "learning_rate": 2.4364123159303884e-05,
      "loss": 3.4537,
      "step": 384
    },
    {
      "epoch": 1.5502008032128514,
      "grad_norm": 2.7950246334075928,
      "learning_rate": 2.4230254350736277e-05,
      "loss": 4.3548,
      "step": 386
    },
    {
      "epoch": 1.5582329317269075,
      "grad_norm": 3.80363392829895,
      "learning_rate": 2.4096385542168677e-05,
      "loss": 3.6184,
      "step": 388
    },
    {
      "epoch": 1.5662650602409638,
      "grad_norm": 2.91756272315979,
      "learning_rate": 2.3962516733601073e-05,
      "loss": 4.8592,
      "step": 390
    },
    {
      "epoch": 1.5742971887550201,
      "grad_norm": 2.2102839946746826,
      "learning_rate": 2.3828647925033466e-05,
      "loss": 3.3943,
      "step": 392
    },
    {
      "epoch": 1.5823293172690764,
      "grad_norm": 2.843629837036133,
      "learning_rate": 2.3694779116465866e-05,
      "loss": 4.0813,
      "step": 394
    },
    {
      "epoch": 1.5903614457831325,
      "grad_norm": 2.5305891036987305,
      "learning_rate": 2.356091030789826e-05,
      "loss": 3.8891,
      "step": 396
    },
    {
      "epoch": 1.5983935742971886,
      "grad_norm": 2.427891731262207,
      "learning_rate": 2.3427041499330656e-05,
      "loss": 4.3758,
      "step": 398
    },
    {
      "epoch": 1.606425702811245,
      "grad_norm": 3.4302098751068115,
      "learning_rate": 2.3293172690763055e-05,
      "loss": 3.733,
      "step": 400
    },
    {
      "epoch": 1.6144578313253013,
      "grad_norm": 3.036206007003784,
      "learning_rate": 2.315930388219545e-05,
      "loss": 3.1722,
      "step": 402
    },
    {
      "epoch": 1.6224899598393574,
      "grad_norm": 3.5680370330810547,
      "learning_rate": 2.3025435073627845e-05,
      "loss": 3.4839,
      "step": 404
    },
    {
      "epoch": 1.6305220883534135,
      "grad_norm": 3.5626630783081055,
      "learning_rate": 2.289156626506024e-05,
      "loss": 4.3562,
      "step": 406
    },
    {
      "epoch": 1.6385542168674698,
      "grad_norm": 2.5315277576446533,
      "learning_rate": 2.2757697456492638e-05,
      "loss": 4.9487,
      "step": 408
    },
    {
      "epoch": 1.6465863453815262,
      "grad_norm": 2.620969772338867,
      "learning_rate": 2.2623828647925034e-05,
      "loss": 4.521,
      "step": 410
    },
    {
      "epoch": 1.6546184738955825,
      "grad_norm": 3.242997169494629,
      "learning_rate": 2.248995983935743e-05,
      "loss": 3.4307,
      "step": 412
    },
    {
      "epoch": 1.6626506024096386,
      "grad_norm": 2.2750244140625,
      "learning_rate": 2.2356091030789827e-05,
      "loss": 4.4845,
      "step": 414
    },
    {
      "epoch": 1.6706827309236947,
      "grad_norm": 2.6739132404327393,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.5493,
      "step": 416
    },
    {
      "epoch": 1.678714859437751,
      "grad_norm": 3.679886817932129,
      "learning_rate": 2.208835341365462e-05,
      "loss": 4.0669,
      "step": 418
    },
    {
      "epoch": 1.6867469879518073,
      "grad_norm": 2.9558472633361816,
      "learning_rate": 2.1954484605087016e-05,
      "loss": 3.7927,
      "step": 420
    },
    {
      "epoch": 1.6947791164658634,
      "grad_norm": 2.675994873046875,
      "learning_rate": 2.1820615796519413e-05,
      "loss": 4.5379,
      "step": 422
    },
    {
      "epoch": 1.7028112449799195,
      "grad_norm": 3.0312585830688477,
      "learning_rate": 2.168674698795181e-05,
      "loss": 3.7265,
      "step": 424
    },
    {
      "epoch": 1.7108433734939759,
      "grad_norm": 3.1824584007263184,
      "learning_rate": 2.1552878179384202e-05,
      "loss": 4.5004,
      "step": 426
    },
    {
      "epoch": 1.7188755020080322,
      "grad_norm": 3.8131558895111084,
      "learning_rate": 2.1419009370816602e-05,
      "loss": 4.6209,
      "step": 428
    },
    {
      "epoch": 1.7269076305220885,
      "grad_norm": 2.3768880367279053,
      "learning_rate": 2.1285140562248998e-05,
      "loss": 3.5394,
      "step": 430
    },
    {
      "epoch": 1.7349397590361446,
      "grad_norm": 1.97145676612854,
      "learning_rate": 2.115127175368139e-05,
      "loss": 3.0218,
      "step": 432
    },
    {
      "epoch": 1.7429718875502007,
      "grad_norm": 2.7154648303985596,
      "learning_rate": 2.101740294511379e-05,
      "loss": 5.4636,
      "step": 434
    },
    {
      "epoch": 1.751004016064257,
      "grad_norm": 3.4818577766418457,
      "learning_rate": 2.0883534136546184e-05,
      "loss": 3.5235,
      "step": 436
    },
    {
      "epoch": 1.7590361445783134,
      "grad_norm": 3.5492873191833496,
      "learning_rate": 2.074966532797858e-05,
      "loss": 4.0157,
      "step": 438
    },
    {
      "epoch": 1.7670682730923695,
      "grad_norm": 2.6729562282562256,
      "learning_rate": 2.061579651941098e-05,
      "loss": 4.0334,
      "step": 440
    },
    {
      "epoch": 1.7751004016064256,
      "grad_norm": 3.422309160232544,
      "learning_rate": 2.0481927710843373e-05,
      "loss": 4.1449,
      "step": 442
    },
    {
      "epoch": 1.783132530120482,
      "grad_norm": 3.114516496658325,
      "learning_rate": 2.034805890227577e-05,
      "loss": 3.7336,
      "step": 444
    },
    {
      "epoch": 1.7911646586345382,
      "grad_norm": 2.985628604888916,
      "learning_rate": 2.0214190093708166e-05,
      "loss": 3.2759,
      "step": 446
    },
    {
      "epoch": 1.7991967871485943,
      "grad_norm": 4.368059158325195,
      "learning_rate": 2.0080321285140562e-05,
      "loss": 3.8928,
      "step": 448
    },
    {
      "epoch": 1.8072289156626506,
      "grad_norm": 4.049910068511963,
      "learning_rate": 1.994645247657296e-05,
      "loss": 5.0584,
      "step": 450
    },
    {
      "epoch": 1.8152610441767068,
      "grad_norm": 2.711061954498291,
      "learning_rate": 1.9812583668005355e-05,
      "loss": 4.073,
      "step": 452
    },
    {
      "epoch": 1.823293172690763,
      "grad_norm": 4.386003017425537,
      "learning_rate": 1.967871485943775e-05,
      "loss": 3.0352,
      "step": 454
    },
    {
      "epoch": 1.8313253012048194,
      "grad_norm": 2.6646711826324463,
      "learning_rate": 1.9544846050870148e-05,
      "loss": 4.4748,
      "step": 456
    },
    {
      "epoch": 1.8393574297188755,
      "grad_norm": 3.3791661262512207,
      "learning_rate": 1.9410977242302544e-05,
      "loss": 3.2619,
      "step": 458
    },
    {
      "epoch": 1.8473895582329316,
      "grad_norm": 3.5677671432495117,
      "learning_rate": 1.927710843373494e-05,
      "loss": 3.6849,
      "step": 460
    },
    {
      "epoch": 1.855421686746988,
      "grad_norm": 2.664116382598877,
      "learning_rate": 1.9143239625167337e-05,
      "loss": 4.2136,
      "step": 462
    },
    {
      "epoch": 1.8634538152610443,
      "grad_norm": 3.3247227668762207,
      "learning_rate": 1.9009370816599734e-05,
      "loss": 3.9326,
      "step": 464
    },
    {
      "epoch": 1.8714859437751004,
      "grad_norm": 2.995849847793579,
      "learning_rate": 1.8875502008032127e-05,
      "loss": 3.8401,
      "step": 466
    },
    {
      "epoch": 1.8795180722891565,
      "grad_norm": 3.6058337688446045,
      "learning_rate": 1.8741633199464527e-05,
      "loss": 4.0291,
      "step": 468
    },
    {
      "epoch": 1.8875502008032128,
      "grad_norm": 3.6630380153656006,
      "learning_rate": 1.8607764390896923e-05,
      "loss": 3.8142,
      "step": 470
    },
    {
      "epoch": 1.895582329317269,
      "grad_norm": 5.135098934173584,
      "learning_rate": 1.8473895582329316e-05,
      "loss": 4.1582,
      "step": 472
    },
    {
      "epoch": 1.9036144578313254,
      "grad_norm": 2.8718488216400146,
      "learning_rate": 1.8340026773761716e-05,
      "loss": 3.9291,
      "step": 474
    },
    {
      "epoch": 1.9116465863453815,
      "grad_norm": 3.8008034229278564,
      "learning_rate": 1.820615796519411e-05,
      "loss": 4.4809,
      "step": 476
    },
    {
      "epoch": 1.9196787148594376,
      "grad_norm": 3.0378119945526123,
      "learning_rate": 1.8072289156626505e-05,
      "loss": 4.1143,
      "step": 478
    },
    {
      "epoch": 1.927710843373494,
      "grad_norm": 4.968132019042969,
      "learning_rate": 1.7938420348058905e-05,
      "loss": 3.2438,
      "step": 480
    },
    {
      "epoch": 1.9357429718875503,
      "grad_norm": 2.590315103530884,
      "learning_rate": 1.7804551539491298e-05,
      "loss": 4.1159,
      "step": 482
    },
    {
      "epoch": 1.9437751004016064,
      "grad_norm": 3.603400707244873,
      "learning_rate": 1.7670682730923694e-05,
      "loss": 3.3129,
      "step": 484
    },
    {
      "epoch": 1.9518072289156625,
      "grad_norm": 2.0774290561676025,
      "learning_rate": 1.753681392235609e-05,
      "loss": 3.9842,
      "step": 486
    },
    {
      "epoch": 1.9598393574297188,
      "grad_norm": 2.9495716094970703,
      "learning_rate": 1.7402945113788487e-05,
      "loss": 4.2811,
      "step": 488
    },
    {
      "epoch": 1.9678714859437751,
      "grad_norm": 2.760579824447632,
      "learning_rate": 1.7269076305220887e-05,
      "loss": 3.4181,
      "step": 490
    },
    {
      "epoch": 1.9759036144578315,
      "grad_norm": 3.307354688644409,
      "learning_rate": 1.713520749665328e-05,
      "loss": 4.5046,
      "step": 492
    },
    {
      "epoch": 1.9839357429718876,
      "grad_norm": 2.7896010875701904,
      "learning_rate": 1.7001338688085676e-05,
      "loss": 3.3017,
      "step": 494
    },
    {
      "epoch": 1.9919678714859437,
      "grad_norm": 4.104861259460449,
      "learning_rate": 1.6867469879518073e-05,
      "loss": 3.4468,
      "step": 496
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.861054420471191,
      "learning_rate": 1.673360107095047e-05,
      "loss": 3.8417,
      "step": 498
    },
    {
      "epoch": 2.0080321285140563,
      "grad_norm": 3.3025693893432617,
      "learning_rate": 1.6599732262382866e-05,
      "loss": 4.6071,
      "step": 500
    },
    {
      "epoch": 2.0160642570281126,
      "grad_norm": 3.4070146083831787,
      "learning_rate": 1.6465863453815262e-05,
      "loss": 4.1204,
      "step": 502
    },
    {
      "epoch": 2.0240963855421685,
      "grad_norm": 3.1775219440460205,
      "learning_rate": 1.633199464524766e-05,
      "loss": 3.1051,
      "step": 504
    },
    {
      "epoch": 2.032128514056225,
      "grad_norm": 2.9860353469848633,
      "learning_rate": 1.619812583668005e-05,
      "loss": 3.1718,
      "step": 506
    },
    {
      "epoch": 2.040160642570281,
      "grad_norm": 2.827540874481201,
      "learning_rate": 1.606425702811245e-05,
      "loss": 3.7626,
      "step": 508
    },
    {
      "epoch": 2.0481927710843375,
      "grad_norm": 2.4583659172058105,
      "learning_rate": 1.5930388219544848e-05,
      "loss": 3.5769,
      "step": 510
    },
    {
      "epoch": 2.0562248995983934,
      "grad_norm": 3.3234293460845947,
      "learning_rate": 1.579651941097724e-05,
      "loss": 5.1322,
      "step": 512
    },
    {
      "epoch": 2.0642570281124497,
      "grad_norm": 3.5570006370544434,
      "learning_rate": 1.566265060240964e-05,
      "loss": 4.6279,
      "step": 514
    },
    {
      "epoch": 2.072289156626506,
      "grad_norm": 3.491497278213501,
      "learning_rate": 1.5528781793842034e-05,
      "loss": 4.3791,
      "step": 516
    },
    {
      "epoch": 2.0803212851405624,
      "grad_norm": 2.809997797012329,
      "learning_rate": 1.5394912985274433e-05,
      "loss": 3.7149,
      "step": 518
    },
    {
      "epoch": 2.0883534136546187,
      "grad_norm": 2.596284866333008,
      "learning_rate": 1.526104417670683e-05,
      "loss": 3.9611,
      "step": 520
    },
    {
      "epoch": 2.0963855421686746,
      "grad_norm": 3.237351179122925,
      "learning_rate": 1.5127175368139224e-05,
      "loss": 4.715,
      "step": 522
    },
    {
      "epoch": 2.104417670682731,
      "grad_norm": 3.7551379203796387,
      "learning_rate": 1.4993306559571621e-05,
      "loss": 3.554,
      "step": 524
    },
    {
      "epoch": 2.112449799196787,
      "grad_norm": 3.2162976264953613,
      "learning_rate": 1.4859437751004016e-05,
      "loss": 3.5858,
      "step": 526
    },
    {
      "epoch": 2.1204819277108435,
      "grad_norm": 2.400238037109375,
      "learning_rate": 1.4725568942436414e-05,
      "loss": 3.8828,
      "step": 528
    },
    {
      "epoch": 2.1285140562248994,
      "grad_norm": 3.424015998840332,
      "learning_rate": 1.459170013386881e-05,
      "loss": 4.2075,
      "step": 530
    },
    {
      "epoch": 2.1365461847389557,
      "grad_norm": 2.5293421745300293,
      "learning_rate": 1.4457831325301205e-05,
      "loss": 4.3079,
      "step": 532
    },
    {
      "epoch": 2.144578313253012,
      "grad_norm": 2.759702444076538,
      "learning_rate": 1.4323962516733603e-05,
      "loss": 4.3542,
      "step": 534
    },
    {
      "epoch": 2.1526104417670684,
      "grad_norm": 5.399381160736084,
      "learning_rate": 1.4190093708165998e-05,
      "loss": 3.2202,
      "step": 536
    },
    {
      "epoch": 2.1606425702811247,
      "grad_norm": 3.4397215843200684,
      "learning_rate": 1.4056224899598394e-05,
      "loss": 4.0357,
      "step": 538
    },
    {
      "epoch": 2.1686746987951806,
      "grad_norm": 2.6696677207946777,
      "learning_rate": 1.3922356091030792e-05,
      "loss": 3.6495,
      "step": 540
    },
    {
      "epoch": 2.176706827309237,
      "grad_norm": 2.8604259490966797,
      "learning_rate": 1.3788487282463187e-05,
      "loss": 3.1465,
      "step": 542
    },
    {
      "epoch": 2.1847389558232932,
      "grad_norm": 3.0543341636657715,
      "learning_rate": 1.3654618473895583e-05,
      "loss": 3.7611,
      "step": 544
    },
    {
      "epoch": 2.1927710843373496,
      "grad_norm": 2.4074199199676514,
      "learning_rate": 1.3520749665327978e-05,
      "loss": 4.5931,
      "step": 546
    },
    {
      "epoch": 2.2008032128514055,
      "grad_norm": 3.348470687866211,
      "learning_rate": 1.3386880856760376e-05,
      "loss": 3.2835,
      "step": 548
    },
    {
      "epoch": 2.208835341365462,
      "grad_norm": 3.3599069118499756,
      "learning_rate": 1.3253012048192772e-05,
      "loss": 4.1337,
      "step": 550
    },
    {
      "epoch": 2.216867469879518,
      "grad_norm": 4.664905548095703,
      "learning_rate": 1.3119143239625167e-05,
      "loss": 4.5654,
      "step": 552
    },
    {
      "epoch": 2.2248995983935744,
      "grad_norm": 3.295567750930786,
      "learning_rate": 1.2985274431057565e-05,
      "loss": 4.407,
      "step": 554
    },
    {
      "epoch": 2.2329317269076308,
      "grad_norm": 2.6005136966705322,
      "learning_rate": 1.285140562248996e-05,
      "loss": 4.9653,
      "step": 556
    },
    {
      "epoch": 2.2409638554216866,
      "grad_norm": 3.664799451828003,
      "learning_rate": 1.2717536813922356e-05,
      "loss": 3.502,
      "step": 558
    },
    {
      "epoch": 2.248995983935743,
      "grad_norm": 3.1377780437469482,
      "learning_rate": 1.2583668005354755e-05,
      "loss": 4.1254,
      "step": 560
    },
    {
      "epoch": 2.2570281124497993,
      "grad_norm": 3.4290733337402344,
      "learning_rate": 1.244979919678715e-05,
      "loss": 3.2452,
      "step": 562
    },
    {
      "epoch": 2.2650602409638556,
      "grad_norm": 3.584749460220337,
      "learning_rate": 1.2315930388219546e-05,
      "loss": 3.5029,
      "step": 564
    },
    {
      "epoch": 2.2730923694779115,
      "grad_norm": 2.5009500980377197,
      "learning_rate": 1.2182061579651942e-05,
      "loss": 3.7514,
      "step": 566
    },
    {
      "epoch": 2.281124497991968,
      "grad_norm": 2.9722023010253906,
      "learning_rate": 1.2048192771084338e-05,
      "loss": 3.8393,
      "step": 568
    },
    {
      "epoch": 2.289156626506024,
      "grad_norm": 3.0329527854919434,
      "learning_rate": 1.1914323962516733e-05,
      "loss": 3.9132,
      "step": 570
    },
    {
      "epoch": 2.2971887550200805,
      "grad_norm": 3.005312442779541,
      "learning_rate": 1.178045515394913e-05,
      "loss": 3.2255,
      "step": 572
    },
    {
      "epoch": 2.305220883534137,
      "grad_norm": 2.332961320877075,
      "learning_rate": 1.1646586345381528e-05,
      "loss": 4.598,
      "step": 574
    },
    {
      "epoch": 2.3132530120481927,
      "grad_norm": 2.931938648223877,
      "learning_rate": 1.1512717536813922e-05,
      "loss": 3.938,
      "step": 576
    },
    {
      "epoch": 2.321285140562249,
      "grad_norm": 3.989997148513794,
      "learning_rate": 1.1378848728246319e-05,
      "loss": 3.8551,
      "step": 578
    },
    {
      "epoch": 2.3293172690763053,
      "grad_norm": 3.5744569301605225,
      "learning_rate": 1.1244979919678715e-05,
      "loss": 4.2546,
      "step": 580
    },
    {
      "epoch": 2.337349397590361,
      "grad_norm": 3.816516399383545,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 3.2721,
      "step": 582
    },
    {
      "epoch": 2.3453815261044175,
      "grad_norm": 3.017686128616333,
      "learning_rate": 1.0977242302543508e-05,
      "loss": 3.916,
      "step": 584
    },
    {
      "epoch": 2.353413654618474,
      "grad_norm": 4.259337425231934,
      "learning_rate": 1.0843373493975904e-05,
      "loss": 3.3364,
      "step": 586
    },
    {
      "epoch": 2.36144578313253,
      "grad_norm": 2.7991085052490234,
      "learning_rate": 1.0709504685408301e-05,
      "loss": 3.6094,
      "step": 588
    },
    {
      "epoch": 2.3694779116465865,
      "grad_norm": 2.992088794708252,
      "learning_rate": 1.0575635876840696e-05,
      "loss": 4.6389,
      "step": 590
    },
    {
      "epoch": 2.3775100401606424,
      "grad_norm": 3.131697177886963,
      "learning_rate": 1.0441767068273092e-05,
      "loss": 4.783,
      "step": 592
    },
    {
      "epoch": 2.3855421686746987,
      "grad_norm": 2.247150182723999,
      "learning_rate": 1.030789825970549e-05,
      "loss": 3.5691,
      "step": 594
    },
    {
      "epoch": 2.393574297188755,
      "grad_norm": 2.657667875289917,
      "learning_rate": 1.0174029451137885e-05,
      "loss": 3.5943,
      "step": 596
    },
    {
      "epoch": 2.4016064257028114,
      "grad_norm": 3.4826884269714355,
      "learning_rate": 1.0040160642570281e-05,
      "loss": 3.9983,
      "step": 598
    },
    {
      "epoch": 2.4096385542168672,
      "grad_norm": 3.8757927417755127,
      "learning_rate": 9.906291834002678e-06,
      "loss": 3.912,
      "step": 600
    },
    {
      "epoch": 2.4176706827309236,
      "grad_norm": 4.204577922821045,
      "learning_rate": 9.772423025435074e-06,
      "loss": 3.9333,
      "step": 602
    },
    {
      "epoch": 2.42570281124498,
      "grad_norm": 2.79374098777771,
      "learning_rate": 9.63855421686747e-06,
      "loss": 4.4338,
      "step": 604
    },
    {
      "epoch": 2.433734939759036,
      "grad_norm": 2.7779016494750977,
      "learning_rate": 9.504685408299867e-06,
      "loss": 3.65,
      "step": 606
    },
    {
      "epoch": 2.4417670682730925,
      "grad_norm": 3.7644875049591064,
      "learning_rate": 9.370816599732263e-06,
      "loss": 3.9921,
      "step": 608
    },
    {
      "epoch": 2.4497991967871484,
      "grad_norm": 4.505722999572754,
      "learning_rate": 9.236947791164658e-06,
      "loss": 3.8746,
      "step": 610
    },
    {
      "epoch": 2.4578313253012047,
      "grad_norm": 3.767899751663208,
      "learning_rate": 9.103078982597054e-06,
      "loss": 3.5135,
      "step": 612
    },
    {
      "epoch": 2.465863453815261,
      "grad_norm": 4.4452385902404785,
      "learning_rate": 8.969210174029452e-06,
      "loss": 3.0225,
      "step": 614
    },
    {
      "epoch": 2.4738955823293174,
      "grad_norm": 2.815237522125244,
      "learning_rate": 8.835341365461847e-06,
      "loss": 3.6547,
      "step": 616
    },
    {
      "epoch": 2.4819277108433733,
      "grad_norm": 2.248382091522217,
      "learning_rate": 8.701472556894244e-06,
      "loss": 3.7737,
      "step": 618
    },
    {
      "epoch": 2.4899598393574296,
      "grad_norm": 2.9540653228759766,
      "learning_rate": 8.56760374832664e-06,
      "loss": 4.1578,
      "step": 620
    },
    {
      "epoch": 2.497991967871486,
      "grad_norm": 3.2357897758483887,
      "learning_rate": 8.433734939759036e-06,
      "loss": 3.9976,
      "step": 622
    },
    {
      "epoch": 2.5060240963855422,
      "grad_norm": 2.4817616939544678,
      "learning_rate": 8.299866131191433e-06,
      "loss": 3.8089,
      "step": 624
    },
    {
      "epoch": 2.5140562248995986,
      "grad_norm": 2.485883951187134,
      "learning_rate": 8.16599732262383e-06,
      "loss": 4.1151,
      "step": 626
    },
    {
      "epoch": 2.522088353413655,
      "grad_norm": 2.529438018798828,
      "learning_rate": 8.032128514056226e-06,
      "loss": 3.8992,
      "step": 628
    },
    {
      "epoch": 2.5301204819277108,
      "grad_norm": 2.342388391494751,
      "learning_rate": 7.89825970548862e-06,
      "loss": 4.3708,
      "step": 630
    },
    {
      "epoch": 2.538152610441767,
      "grad_norm": 2.6290106773376465,
      "learning_rate": 7.764390896921017e-06,
      "loss": 4.6936,
      "step": 632
    },
    {
      "epoch": 2.5461847389558234,
      "grad_norm": 2.6988589763641357,
      "learning_rate": 7.630522088353415e-06,
      "loss": 3.8941,
      "step": 634
    },
    {
      "epoch": 2.5542168674698793,
      "grad_norm": 2.0697033405303955,
      "learning_rate": 7.4966532797858104e-06,
      "loss": 3.434,
      "step": 636
    },
    {
      "epoch": 2.5622489959839356,
      "grad_norm": 4.389152526855469,
      "learning_rate": 7.362784471218207e-06,
      "loss": 4.2792,
      "step": 638
    },
    {
      "epoch": 2.570281124497992,
      "grad_norm": 2.8002233505249023,
      "learning_rate": 7.228915662650602e-06,
      "loss": 4.5366,
      "step": 640
    },
    {
      "epoch": 2.5783132530120483,
      "grad_norm": 3.374284267425537,
      "learning_rate": 7.095046854082999e-06,
      "loss": 3.2341,
      "step": 642
    },
    {
      "epoch": 2.5863453815261046,
      "grad_norm": 2.231255054473877,
      "learning_rate": 6.961178045515396e-06,
      "loss": 4.1524,
      "step": 644
    },
    {
      "epoch": 2.5943775100401605,
      "grad_norm": 2.769995927810669,
      "learning_rate": 6.827309236947792e-06,
      "loss": 3.9721,
      "step": 646
    },
    {
      "epoch": 2.602409638554217,
      "grad_norm": 3.7699854373931885,
      "learning_rate": 6.693440428380188e-06,
      "loss": 2.7815,
      "step": 648
    },
    {
      "epoch": 2.610441767068273,
      "grad_norm": 2.2925667762756348,
      "learning_rate": 6.559571619812584e-06,
      "loss": 4.0978,
      "step": 650
    },
    {
      "epoch": 2.6184738955823295,
      "grad_norm": 2.9078493118286133,
      "learning_rate": 6.42570281124498e-06,
      "loss": 3.1783,
      "step": 652
    },
    {
      "epoch": 2.6265060240963853,
      "grad_norm": 3.327785015106201,
      "learning_rate": 6.291834002677377e-06,
      "loss": 4.058,
      "step": 654
    },
    {
      "epoch": 2.6345381526104417,
      "grad_norm": 3.496631383895874,
      "learning_rate": 6.157965194109773e-06,
      "loss": 3.4768,
      "step": 656
    },
    {
      "epoch": 2.642570281124498,
      "grad_norm": 3.0983426570892334,
      "learning_rate": 6.024096385542169e-06,
      "loss": 3.6339,
      "step": 658
    },
    {
      "epoch": 2.6506024096385543,
      "grad_norm": 3.225945472717285,
      "learning_rate": 5.890227576974565e-06,
      "loss": 3.4525,
      "step": 660
    },
    {
      "epoch": 2.6586345381526106,
      "grad_norm": 3.446134328842163,
      "learning_rate": 5.756358768406961e-06,
      "loss": 4.1681,
      "step": 662
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.955927848815918,
      "learning_rate": 5.622489959839358e-06,
      "loss": 2.9232,
      "step": 664
    },
    {
      "epoch": 2.674698795180723,
      "grad_norm": 3.809448719024658,
      "learning_rate": 5.488621151271754e-06,
      "loss": 4.9192,
      "step": 666
    },
    {
      "epoch": 2.682730923694779,
      "grad_norm": 3.4281725883483887,
      "learning_rate": 5.3547523427041504e-06,
      "loss": 3.2061,
      "step": 668
    },
    {
      "epoch": 2.6907630522088355,
      "grad_norm": 2.5807981491088867,
      "learning_rate": 5.220883534136546e-06,
      "loss": 3.4539,
      "step": 670
    },
    {
      "epoch": 2.6987951807228914,
      "grad_norm": 2.450402021408081,
      "learning_rate": 5.087014725568942e-06,
      "loss": 3.9319,
      "step": 672
    },
    {
      "epoch": 2.7068273092369477,
      "grad_norm": 3.4279611110687256,
      "learning_rate": 4.953145917001339e-06,
      "loss": 3.8275,
      "step": 674
    },
    {
      "epoch": 2.714859437751004,
      "grad_norm": 2.9377386569976807,
      "learning_rate": 4.819277108433735e-06,
      "loss": 3.4898,
      "step": 676
    },
    {
      "epoch": 2.7228915662650603,
      "grad_norm": 3.350083589553833,
      "learning_rate": 4.685408299866132e-06,
      "loss": 4.5787,
      "step": 678
    },
    {
      "epoch": 2.7309236947791167,
      "grad_norm": 4.241281032562256,
      "learning_rate": 4.551539491298527e-06,
      "loss": 4.3798,
      "step": 680
    },
    {
      "epoch": 2.7389558232931726,
      "grad_norm": 3.6463348865509033,
      "learning_rate": 4.417670682730924e-06,
      "loss": 4.3736,
      "step": 682
    },
    {
      "epoch": 2.746987951807229,
      "grad_norm": 3.686828851699829,
      "learning_rate": 4.28380187416332e-06,
      "loss": 3.6079,
      "step": 684
    },
    {
      "epoch": 2.755020080321285,
      "grad_norm": 3.1928274631500244,
      "learning_rate": 4.149933065595716e-06,
      "loss": 2.7294,
      "step": 686
    },
    {
      "epoch": 2.7630522088353415,
      "grad_norm": 2.714991807937622,
      "learning_rate": 4.016064257028113e-06,
      "loss": 4.4873,
      "step": 688
    },
    {
      "epoch": 2.7710843373493974,
      "grad_norm": 3.620051383972168,
      "learning_rate": 3.882195448460508e-06,
      "loss": 4.2455,
      "step": 690
    },
    {
      "epoch": 2.7791164658634537,
      "grad_norm": 4.55225133895874,
      "learning_rate": 3.7483266398929052e-06,
      "loss": 4.6626,
      "step": 692
    },
    {
      "epoch": 2.78714859437751,
      "grad_norm": 4.447392463684082,
      "learning_rate": 3.614457831325301e-06,
      "loss": 3.7397,
      "step": 694
    },
    {
      "epoch": 2.7951807228915664,
      "grad_norm": 3.759800672531128,
      "learning_rate": 3.480589022757698e-06,
      "loss": 4.1689,
      "step": 696
    },
    {
      "epoch": 2.8032128514056227,
      "grad_norm": 3.730489730834961,
      "learning_rate": 3.346720214190094e-06,
      "loss": 3.657,
      "step": 698
    },
    {
      "epoch": 2.8112449799196786,
      "grad_norm": 3.909412384033203,
      "learning_rate": 3.21285140562249e-06,
      "loss": 3.2579,
      "step": 700
    },
    {
      "epoch": 2.819277108433735,
      "grad_norm": 2.9406700134277344,
      "learning_rate": 3.0789825970548864e-06,
      "loss": 4.2652,
      "step": 702
    },
    {
      "epoch": 2.8273092369477912,
      "grad_norm": 3.7192232608795166,
      "learning_rate": 2.9451137884872824e-06,
      "loss": 4.3846,
      "step": 704
    },
    {
      "epoch": 2.835341365461847,
      "grad_norm": 2.9725944995880127,
      "learning_rate": 2.811244979919679e-06,
      "loss": 4.5199,
      "step": 706
    },
    {
      "epoch": 2.8433734939759034,
      "grad_norm": 3.170762062072754,
      "learning_rate": 2.6773761713520752e-06,
      "loss": 3.2744,
      "step": 708
    },
    {
      "epoch": 2.8514056224899598,
      "grad_norm": 2.761179208755493,
      "learning_rate": 2.543507362784471e-06,
      "loss": 3.5523,
      "step": 710
    },
    {
      "epoch": 2.859437751004016,
      "grad_norm": 3.295090436935425,
      "learning_rate": 2.4096385542168676e-06,
      "loss": 4.7624,
      "step": 712
    },
    {
      "epoch": 2.8674698795180724,
      "grad_norm": 3.554670572280884,
      "learning_rate": 2.2757697456492636e-06,
      "loss": 3.8927,
      "step": 714
    },
    {
      "epoch": 2.8755020080321287,
      "grad_norm": 2.7626559734344482,
      "learning_rate": 2.14190093708166e-06,
      "loss": 4.1793,
      "step": 716
    },
    {
      "epoch": 2.8835341365461846,
      "grad_norm": 3.3145878314971924,
      "learning_rate": 2.0080321285140564e-06,
      "loss": 3.9353,
      "step": 718
    },
    {
      "epoch": 2.891566265060241,
      "grad_norm": 2.564634084701538,
      "learning_rate": 1.8741633199464526e-06,
      "loss": 3.575,
      "step": 720
    },
    {
      "epoch": 2.8995983935742973,
      "grad_norm": 2.9032013416290283,
      "learning_rate": 1.740294511378849e-06,
      "loss": 3.8259,
      "step": 722
    },
    {
      "epoch": 2.907630522088353,
      "grad_norm": 2.817664384841919,
      "learning_rate": 1.606425702811245e-06,
      "loss": 4.0065,
      "step": 724
    },
    {
      "epoch": 2.9156626506024095,
      "grad_norm": 3.0890555381774902,
      "learning_rate": 1.4725568942436412e-06,
      "loss": 3.5443,
      "step": 726
    },
    {
      "epoch": 2.923694779116466,
      "grad_norm": 3.204047918319702,
      "learning_rate": 1.3386880856760376e-06,
      "loss": 4.7146,
      "step": 728
    },
    {
      "epoch": 2.931726907630522,
      "grad_norm": 3.7792985439300537,
      "learning_rate": 1.2048192771084338e-06,
      "loss": 4.0424,
      "step": 730
    },
    {
      "epoch": 2.9397590361445785,
      "grad_norm": 2.5436949729919434,
      "learning_rate": 1.07095046854083e-06,
      "loss": 4.4407,
      "step": 732
    },
    {
      "epoch": 2.9477911646586348,
      "grad_norm": 3.1278600692749023,
      "learning_rate": 9.370816599732263e-07,
      "loss": 3.8447,
      "step": 734
    },
    {
      "epoch": 2.9558232931726907,
      "grad_norm": 4.127079010009766,
      "learning_rate": 8.032128514056225e-07,
      "loss": 3.6525,
      "step": 736
    },
    {
      "epoch": 2.963855421686747,
      "grad_norm": 3.916755437850952,
      "learning_rate": 6.693440428380188e-07,
      "loss": 3.4391,
      "step": 738
    },
    {
      "epoch": 2.9718875502008033,
      "grad_norm": 2.2839086055755615,
      "learning_rate": 5.35475234270415e-07,
      "loss": 3.5992,
      "step": 740
    },
    {
      "epoch": 2.979919678714859,
      "grad_norm": 3.1556496620178223,
      "learning_rate": 4.0160642570281125e-07,
      "loss": 3.7349,
      "step": 742
    },
    {
      "epoch": 2.9879518072289155,
      "grad_norm": 2.682687759399414,
      "learning_rate": 2.677376171352075e-07,
      "loss": 3.4489,
      "step": 744
    },
    {
      "epoch": 2.995983935742972,
      "grad_norm": 3.121490478515625,
      "learning_rate": 1.3386880856760375e-07,
      "loss": 4.0113,
      "step": 746
    }
  ],
  "logging_steps": 2,
  "max_steps": 747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 48868031987712.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
